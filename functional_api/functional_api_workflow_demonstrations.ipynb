{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LangGraph Functional API Workflow Demonstrations\n",
        "\n",
        "This notebook demonstrates the key capabilities of LangGraph's Functional API approach, showing how it differs from traditional graph-based and event-driven architectures.\n",
        "\n",
        "## Architecture Paradigm: Function-based with @entrypoint and @task decorators\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function-based with @entrypoint and @task decorators\n",
        "from langgraph.func import entrypoint, task\n",
        "\n",
        "@task()\n",
        "def process_data(input_data: str) -> dict:\n",
        "    # Standard Python constructs - no graph thinking\n",
        "    if input_data.startswith(\"urgent\"):\n",
        "        return {\"priority\": \"high\", \"data\": input_data}\n",
        "    return {\"priority\": \"normal\", \"data\": input_data}\n",
        "\n",
        "@entrypoint()\n",
        "def main_workflow(user_input: str) -> dict:\n",
        "    # Imperative programming style\n",
        "    result = process_data(user_input).result()\n",
        "    return result\n",
        "\n",
        "print(\"ðŸ”§ Functional API workflow created - function-based approach with decorators\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Control Flow: Standard Python constructs (if/for/while loops)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard Python constructs (if/for/while loops)\n",
        "@entrypoint()\n",
        "def control_flow_workflow(data_list: list) -> list:\n",
        "    processed_items = []\n",
        "    \n",
        "    for item in data_list:  # Standard Python for loop\n",
        "        if item > 10:  # Standard Python if statement\n",
        "            processed_items.append(item * 2)\n",
        "        elif item < 0:\n",
        "            continue  # Standard Python control flow\n",
        "            \n",
        "    return processed_items\n",
        "\n",
        "print(\"ðŸ”§ Control flow workflow created - standard Python constructs, no graph thinking\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Observability: State changes and variable changes within steps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# State changes and variable changes within steps\n",
        "from datetime import datetime\n",
        "\n",
        "@task()\n",
        "def observable_task(input_data: dict) -> dict:\n",
        "    # State changes are tracked automatically\n",
        "    input_data[\"processed\"] = True\n",
        "    input_data[\"timestamp\"] = datetime.now()\n",
        "    \n",
        "    # Variable changes within steps are observable\n",
        "    counter = 0\n",
        "    for item in input_data.get(\"items\", []):\n",
        "        counter += 1\n",
        "        input_data[f\"item_{counter}\"] = item\n",
        "        \n",
        "    return input_data\n",
        "\n",
        "print(\"ðŸ”§ Observable task created - state changes and variable tracking within steps\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Checkpointing: Entrypoint-level checkpointing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entrypoint-level checkpointing\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "@entrypoint(checkpointer=MemorySaver())\n",
        "def checkpointed_workflow(input_data: str, config: dict) -> str:\n",
        "    # Checkpointing happens at entrypoint level\n",
        "    step1_result = process_step1(input_data).result()\n",
        "    \n",
        "    # Resume after error by running with None and same thread_id\n",
        "    step2_result = process_step2(step1_result).result()\n",
        "    \n",
        "    return step2_result\n",
        "\n",
        "@task()\n",
        "def process_step1(data: str) -> str:\n",
        "    return f\"processed_{data}\"\n",
        "\n",
        "@task()\n",
        "def process_step2(data: str) -> str:\n",
        "    return f\"final_{data}\"\n",
        "\n",
        "print(\"ðŸ”§ Checkpointed workflow created - entrypoint-level checkpointing\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Human-in-the-Loop: Capability available but with limitations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Human-in-the-loop capability with limitations\n",
        "@task()\n",
        "def human_review_task(document: str) -> str:\n",
        "    # Limited human interaction - requires external input mechanism\n",
        "    print(f\"Document for review: {document}\")\n",
        "    user_input = input(\"Approve? (y/n): \")\n",
        "    \n",
        "    if user_input.lower() == 'y':\n",
        "        return \"approved\"\n",
        "    return \"rejected\"\n",
        "\n",
        "@entrypoint()\n",
        "def human_workflow(document: str) -> str:\n",
        "    result = human_review_task(document).result()\n",
        "    return f\"Document status: {result}\"\n",
        "\n",
        "print(\"ðŸ”§ Human-in-the-loop workflow created - limited but available human interaction\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Node Parallelization: Same step execution with different data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Same step can be executed in parallel with different data\n",
        "import asyncio\n",
        "\n",
        "@task()\n",
        "async def parallel_task(item: str) -> str:\n",
        "    # Same step with different data concurrently\n",
        "    await asyncio.sleep(1)  # Simulate processing\n",
        "    return f\"processed_{item}\"\n",
        "\n",
        "@entrypoint()\n",
        "async def parallel_workflow(items: list) -> list:\n",
        "    # Execute same step in parallel with different data\n",
        "    tasks = [parallel_task(item) for item in items]\n",
        "    results = await asyncio.gather(*tasks)\n",
        "    return results\n",
        "\n",
        "print(\"ðŸ”§ Parallel workflow created - same step execution with different data concurrently\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Workflow Composition: Function-based composition using standard Python async/await patterns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function-based composition using standard Python async/await\n",
        "@task()\n",
        "async def task_a(data: str) -> str:\n",
        "    return f\"task_a_{data}\"\n",
        "\n",
        "@task()\n",
        "async def task_b(data: str) -> str:\n",
        "    return f\"task_b_{data}\"\n",
        "\n",
        "@entrypoint()\n",
        "async def composed_workflow(input_data: str) -> dict:\n",
        "    # Sequential composition\n",
        "    result_a = await task_a(input_data).result()\n",
        "    result_b = await task_b(result_a).result()\n",
        "    \n",
        "    # Parallel composition\n",
        "    parallel_results = await asyncio.gather(\n",
        "        task_a(input_data).result(),\n",
        "        task_b(input_data).result()\n",
        "    )\n",
        "    \n",
        "    return {\"sequential\": result_b, \"parallel\": parallel_results}\n",
        "\n",
        "print(\"ðŸ”§ Composed workflow created - function-based composition with async/await patterns\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Error Handling: Resume after error by running with None and same thread ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Resume after error by running with None and same thread ID\n",
        "@entrypoint(checkpointer=MemorySaver())\n",
        "def error_handling_workflow(input_data: str, config: dict) -> str:\n",
        "    try:\n",
        "        result = risky_operation(input_data).result()\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        # Resume after error by running with None and same thread_id\n",
        "        print(f\"Error occurred: {e}\")\n",
        "        # System automatically handles resume with same thread_id\n",
        "        return \"error_recovered\"\n",
        "\n",
        "@task()\n",
        "def risky_operation(data: str) -> str:\n",
        "    import random\n",
        "    if random.random() < 0.5:  # 50% chance of failure\n",
        "        raise Exception(\"Simulated error\")\n",
        "    return f\"success_{data}\"\n",
        "\n",
        "print(\"ðŸ”§ Error handling workflow created - resume after error with same thread ID\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Database Integration: PostgreSQL, Redis, and SQLite integrations available\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PostgreSQL, Redis, SQLite integrations available\n",
        "from langgraph.checkpoint.postgres import PostgresSaver\n",
        "from langgraph.checkpoint.redis import RedisSaver\n",
        "from langgraph.checkpoint.sqlite import SqliteSaver\n",
        "\n",
        "# PostgreSQL integration\n",
        "@entrypoint(checkpointer=PostgresSaver.from_conn_string(\"postgresql://user:pass@localhost/db\"))\n",
        "def postgres_workflow(input_data: str) -> str:\n",
        "    return f\"stored_in_postgres_{input_data}\"\n",
        "\n",
        "# Redis integration  \n",
        "@entrypoint(checkpointer=RedisSaver.from_conn_string(\"redis://localhost:6379\"))\n",
        "def redis_workflow(input_data: str) -> str:\n",
        "    return f\"stored_in_redis_{input_data}\"\n",
        "\n",
        "# SQLite integration\n",
        "@entrypoint(checkpointer=SqliteSaver.from_conn_string(\"sqlite:///checkpoints.db\"))\n",
        "def sqlite_workflow(input_data: str) -> str:\n",
        "    return f\"stored_in_sqlite_{input_data}\"\n",
        "\n",
        "print(\"ðŸ”§ Database integration workflows created - PostgreSQL, Redis, SQLite support\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrates the key capabilities of LangGraph's Functional API:\n",
        "\n",
        "- **Function-based architecture** with `@entrypoint` and `@task` decorators\n",
        "- **Standard Python control flow** (if/for/while loops) without graph thinking\n",
        "- **Step-level observability** with automatic state and variable tracking\n",
        "- **Entrypoint-level checkpointing** with resume capabilities\n",
        "- **Limited human-in-the-loop** integration\n",
        "- **Parallel execution** of same steps with different data\n",
        "- **Function-based composition** using async/await patterns\n",
        "- **Error handling** with automatic resume mechanisms\n",
        "- **Database integration** support for PostgreSQL, Redis, and SQLite\n",
        "\n",
        "The Functional API provides a more imperative, Python-native approach compared to graph-based or event-driven architectures.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
